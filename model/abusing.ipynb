{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codertimo/capstone-2022-08/env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jason9693/soongsil-bert-small were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at jason9693/soongsil-bert-small and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jason9693/soongsil-bert-small\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"jason9693/soongsil-bert-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset kor_hate (/home/codertimo/.cache/huggingface/datasets/kor_hate/default/1.1.0/9362ba4de9bf62ad689be73f9d39f851d8bdc67c3245085d07cf65f972dba95b)\n",
      "100%|██████████| 2/2 [00:00<00:00, 506.71it/s]\n",
      "Using custom data configuration smilegate-ai--kor_unsmile-1dba960877497f9f\n",
      "Reusing dataset parquet (/home/codertimo/.cache/huggingface/datasets/parquet/smilegate-ai--kor_unsmile-1dba960877497f9f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 2/2 [00:00<00:00, 457.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kor_hate_dataset = load_dataset(\"kor_hate\")\n",
    "kor_unsmile_dataset = load_dataset(\"smilegate-ai/kor_unsmile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': '100년안에 남녀간 성전쟁 한번 크게 치룬 후 일부다처제, 여성의 정치참여 금지, 여성 투표권 삭제가 세계의 공통문화로 자리잡을듯. 암탉이 너무 울어댐.',\n",
       " 'contain_gender_bias': 1,\n",
       " 'bias': 1,\n",
       " 'hate': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_hate_dataset[\"train\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'문장': '일안하는 시간은 쉬고싶어서 그런게 아닐까',\n",
       " '여성/가족': 0,\n",
       " '남성': 0,\n",
       " '성소수자': 0,\n",
       " '인종/국적': 0,\n",
       " '연령': 0,\n",
       " '지역': 0,\n",
       " '종교': 0,\n",
       " '기타 혐오': 0,\n",
       " '악플/욕설': 0,\n",
       " 'clean': 1,\n",
       " '개인지칭': 0,\n",
       " 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_unsmile_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = []\n",
    "for data in kor_hate_dataset[\"train\"]:\n",
    "    text = data[\"comments\"]\n",
    "    is_bad = bool(data[\"contain_gender_bias\"] + data[\"bias\"] + data[\"hate\"])\n",
    "    merged_dataset.append({\"text\": text, \"is_bad\": is_bad})\n",
    "for data in kor_unsmile_dataset[\"train\"]:\n",
    "    text = data[\"문장\"]\n",
    "    is_bad = not bool(data[\"clean\"])\n",
    "    merged_dataset.append({\"text\": text, \"is_bad\": is_bad})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22901"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Any, Dict, Tuple\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class AbusingDataset(Dataset):\n",
    "    def __init__(self, data_list: List[Dict[str, Any]], tokenizer: PreTrainedTokenizerFast, max_length: int = 64):\n",
    "        self.data_list = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data_list[index]\n",
    "        tokenized_text = self.tokenizer(data[\"text\"], max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        label = torch.tensor(int(data[\"is_bad\"]))\n",
    "        return tokenized_text.input_ids.squeeze(0), tokenized_text.attention_mask.squeeze(0), label\n",
    "\n",
    "def collate_fn(inputs: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]):\n",
    "    padded_input_ids = pad_sequence([input_ids for input_ids, _, _ in inputs]).transpose(0, 1)\n",
    "    padded_attention_mask = pad_sequence([attention_mask for _, attention_mask, _ in inputs]).transpose(0, 1)\n",
    "    labels = torch.stack([label for _, _, label in inputs])\n",
    "    return padded_input_ids, padded_attention_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step: 100/1432 loss: 0.4342561960220337 acc: 0.8203914141414141\n",
      "epoch: 0 step: 200/1432 loss: 0.36914733052253723 acc: 0.8425\n",
      "epoch: 0 step: 300/1432 loss: 0.36581411957740784 acc: 0.8375\n",
      "epoch: 0 step: 400/1432 loss: 0.35316985845565796 acc: 0.848125\n",
      "epoch: 0 step: 500/1432 loss: 0.3507958948612213 acc: 0.845\n",
      "epoch: 0 step: 600/1432 loss: 0.344075471162796 acc: 0.851875\n",
      "epoch: 0 step: 700/1432 loss: 0.330769419670105 acc: 0.850625\n",
      "epoch: 1 step: 800/1432 loss: 0.24692405760288239 acc: 0.9046723110693007\n",
      "epoch: 1 step: 900/1432 loss: 0.2454180121421814 acc: 0.900625\n",
      "epoch: 1 step: 1000/1432 loss: 0.235290065407753 acc: 0.908125\n",
      "epoch: 1 step: 1100/1432 loss: 0.23407284915447235 acc: 0.909375\n",
      "epoch: 1 step: 1200/1432 loss: 0.23416663706302643 acc: 0.905\n",
      "epoch: 1 step: 1300/1432 loss: 0.23853227496147156 acc: 0.9003125\n",
      "epoch: 1 step: 1400/1432 loss: 0.21741698682308197 acc: 0.91375\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "abusing_dataset = AbusingDataset(merged_dataset, tokenizer)\n",
    "dataloader = DataLoader(abusing_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "EPOCHS = 2\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "global_step = 1\n",
    "total_steps = len(dataloader) * EPOCHS\n",
    "interval_loss = []\n",
    "interval_corrects = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for inputs in dataloader:\n",
    "        input_ids, attention_mask, labels = (tensor.to(device) for tensor in inputs)\n",
    "        model_output = model(input_ids, attention_mask, labels=labels)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        model_output.loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = torch.mean(torch.stack(interval_loss))\n",
    "            avg_acc = np.mean(interval_corrects)\n",
    "            print(f\"epoch: {epoch} step: {global_step}/{total_steps} loss: {avg_loss} acc: {avg_acc}\")\n",
    "            interval_loss.clear()\n",
    "            interval_corrects.clear()\n",
    "        global_step += 1\n",
    "        interval_loss.append(model_output.loss)\n",
    "        interval_corrects.extend(model_output.logits.argmax(-1).eq(labels).long().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bad_text(input_text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer(input_text, max_length=64, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = model_inputs.input_ids.to(device)\n",
    "        attention_mask = model_inputs.attention_mask.to(device)\n",
    "        model_output = model(input_ids, attention_mask)\n",
    "        output_argmax = model_output.logits.argmax(-1)[0]\n",
    "        prob = model_output.logits[0].softmax(-1)[1]\n",
    "    model.train()\n",
    "    return {\"text\": input_text, \"bad_text_prob\": prob.item(), \"is_bad\": output_argmax.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '와 진짜 맛있어 보임 ㄹㅇ ㅋㅋㅋㅋ', 'bad_text_prob': 0.16767928004264832, 'is_bad': 0}\n",
      "{'text': '헐 대박 그거 어떻게 해?', 'bad_text_prob': 0.12916354835033417, 'is_bad': 0}\n",
      "{'text': '또라이 새끼', 'bad_text_prob': 0.9777447581291199, 'is_bad': 1}\n",
      "{'text': 'ㅉㅉ 내가 발로해도 그것보단 잘하겠다', 'bad_text_prob': 0.6949053406715393, 'is_bad': 1}\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"와 진짜 맛있어 보임 ㄹㅇ ㅋㅋㅋㅋ\", \"헐 대박 그거 어떻게 해?\", \"또라이 새끼\", \"ㅉㅉ 내가 발로해도 그것보단 잘하겠다\"]\n",
    "for text in test_texts:\n",
    "    print(check_bad_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0ddbc76db758f3b71330a722750ace758a01b0029cf355db76f48eae0bca05f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
